import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import weight_norm, remove_weight_norm
import numpy as np
import onnxruntime as ort
import argparse
from pathlib import Path
import warnings
import math

warnings.filterwarnings("ignore")

# ==============================================================================
# å®Œæ•´çš„æ¨¡å‹å®šä¹‰ï¼ˆä» export_onnx.py å¤åˆ¶ï¼‰
# ==============================================================================

def kaiser_sinc_filter1d(cutoff, half_width, kernel_size):
    even = (kernel_size % 2 == 0)
    half_size = kernel_size // 2
    delta_f = 4 * half_width
    A = 2.285 * (half_size - 1) * math.pi * delta_f + 7.95
    if A > 50.: 
        beta = 0.1102 * (A - 8.7)
    elif A >= 21.: 
        beta = 0.5842 * (A - 21)**0.4 + 0.07886 * (A - 21.)
    else: 
        beta = 0.
    window = torch.kaiser_window(kernel_size, beta=beta, periodic=False)
    time = (torch.arange(-half_size, half_size) + 0.5) if even else (torch.arange(kernel_size) - half_size)
    filter_ = 2 * cutoff * window * torch.sinc(2 * cutoff * time)
    filter_ /= filter_.sum()
    return filter_.view(1, 1, kernel_size)

class UpSample1d(nn.Module):
    def __init__(self, ratio=2, kernel_size=12, channels=512):
        super().__init__()
        self.ratio = ratio
        self.kernel_size = kernel_size
        self.channels = channels
        
        cutoff = 0.5 / ratio
        half_width = 0.5 / ratio
        filter_ = kaiser_sinc_filter1d(cutoff, half_width, kernel_size)
        
        w = filter_.view(kernel_size) * ratio
        p0, p1 = w[0::2], w[1::2]
        weight = torch.stack([p0, p1], dim=0).unsqueeze(1)
        weight = weight.repeat(channels, 1, 1)
        
        self.conv = nn.Conv1d(
            in_channels=channels,
            out_channels=channels * ratio,
            kernel_size=weight.shape[2],
            stride=1,
            padding=0,
            groups=channels,
            bias=False
        )
        
        with torch.no_grad():
            self.conv.weight.copy_(weight)
        self.conv.weight.requires_grad = False

    def forward(self, x):
        x = F.pad(x, (2, 3), mode='constant', value=0.0)
        out = self.conv(x)
        out = out.view(x.shape[0], x.shape[1], self.ratio, -1)
        out = out.transpose(2, 3).reshape(x.shape[0], x.shape[1], -1)
        out = out[..., 2:-2]
        return out

class LowPassFilter1d(nn.Module):
    def __init__(self, stride=2, kernel_size=12, channels=512):
        super().__init__()
        self.stride = stride
        self.channels = channels
        
        cutoff = 0.5 / stride
        half_width = 0.5 / stride
        filter_ = kaiser_sinc_filter1d(cutoff, half_width, kernel_size)
        filter_ = filter_.repeat(channels, 1, 1)
        
        self.conv = nn.Conv1d(
            in_channels=channels,
            out_channels=channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=5,
            groups=channels,
            bias=False
        )
        
        with torch.no_grad():
            self.conv.weight.copy_(filter_)
        self.conv.weight.requires_grad = False

    def forward(self, x):
        return self.conv(x)

class DownSample1d(nn.Module):
    def __init__(self, ratio=2, kernel_size=12, channels=512):
        super().__init__()
        self.lowpass = LowPassFilter1d(stride=ratio, kernel_size=kernel_size, channels=channels)
    
    def forward(self, x):
        return self.lowpass(x)

class SnakeBeta(nn.Module):
    def __init__(self, in_features, alpha=1.0, alpha_trainable=True, alpha_logscale=False):
        super().__init__()
        self.alpha_logscale = alpha_logscale
        init_val = torch.zeros(in_features) if alpha_logscale else torch.ones(in_features)
        self.alpha = nn.Parameter(init_val * alpha)
        self.beta = nn.Parameter(init_val * alpha)
        self.alpha.requires_grad = alpha_trainable
        self.beta.requires_grad = alpha_trainable

    def forward(self, x):
        if self.alpha_logscale:
            a = torch.exp(self.alpha)
            b = torch.exp(self.beta)
        else:
            a = self.alpha
            b = self.beta
        a = a.view(1, -1, 1)
        b = b.view(1, -1, 1)
        eps = 1e-9
        return x + (1.0 - torch.cos(2.0 * a * x)) / (2.0 * b + eps)

class Activation1d(nn.Module):
    def __init__(self, activation, up_ratio=2, down_ratio=2, 
                 up_kernel_size=12, down_kernel_size=12, channels=512):
        super().__init__()
        self.up_ratio = up_ratio
        self.down_ratio = down_ratio
        self.act = activation
        self.upsample = UpSample1d(up_ratio, up_kernel_size, channels)
        self.downsample = DownSample1d(down_ratio, down_kernel_size, channels)

    def forward(self, x):
        x = self.upsample(x)
        x = self.act(x)
        x = self.downsample(x)
        return x

def get_padding(kernel_size, dilation=1):
    return (kernel_size * dilation - dilation) // 2

class AMPBlock0(nn.Module):
    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5), activation=None):
        super().__init__()
        self.convs1 = nn.ModuleList([
            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, 
                                  dilation=dilation[0],
                                  padding=get_padding(kernel_size, dilation[0])))
        ])
        self.convs2 = nn.ModuleList([
            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, 
                                  dilation=1,
                                  padding=get_padding(kernel_size, 1)))
        ])
        self.num_layers = len(self.convs1) + len(self.convs2)
        self.activations = nn.ModuleList([
            Activation1d(
                activation=SnakeBeta(channels, alpha_logscale=True),
                channels=channels
            ) for _ in range(self.num_layers)
        ])

    def forward(self, x):
        for c1, c2, a1, a2 in zip(self.convs1, self.convs2, 
                                   self.activations[::2], self.activations[1::2]):
            xt = a1(x)
            xt = c1(xt)
            xt = a2(xt)
            xt = c2(xt)
            x_residual = x.narrow(2, 0, xt.shape[2])
            x = xt + x_residual
        return x

    def remove_weight_norm(self):
        for l in self.convs1:
            remove_weight_norm(l)
        for l in self.convs2:
            remove_weight_norm(l)

class Generator(nn.Module):
    def __init__(self, initial_channel, resblock, resblock_kernel_sizes,
                 resblock_dilation_sizes, upsample_initial_channel, gin_channels=0):
        super().__init__()
        self.conv_pre = nn.Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)
        self.resblocks = nn.ModuleList()
        for i in range(1):
            ch = upsample_initial_channel // (2 ** i)
            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):
                self.resblocks.append(AMPBlock0(ch, k, d, activation="snakebeta"))
        self.conv_post = nn.Conv1d(ch, 1, 7, 1, padding=3, bias=False)
        if gin_channels != 0:
            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)

    def forward(self, x, g=None):
        x = self.conv_pre(x)
        x = F.interpolate(x, scale_factor=3.0, mode='linear', align_corners=False)
        xs = self.resblocks[0](x)
        x = self.conv_post(xs)
        x = torch.tanh(x)
        return x

    def remove_weight_norm(self):
        for l in self.resblocks:
            l.remove_weight_norm()

class SynthesizerTrn(nn.Module):
    def __init__(self, spec_channels, segment_size, resblock,
                 resblock_kernel_sizes, resblock_dilation_sizes, 
                 upsample_initial_channel):
        super().__init__()
        self.spec_channels = spec_channels
        self.segment_size = segment_size
        self.dec = Generator(
            1, resblock, resblock_kernel_sizes,
            resblock_dilation_sizes, upsample_initial_channel
        )

    def forward(self, x):
        return self.dec(x)

# ==============================================================================
# æƒé‡è½¬æ¢å‡½æ•°
# ==============================================================================

def convert_state_dict(state_dict, model_channels):
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = k.replace('module.', '')
        if 'upsample.filter' in new_key:
            ratio = 2
            kernel_size = v.shape[2]
            w = v.view(kernel_size) * ratio
            p0, p1 = w[0::2], w[1::2]
            weight = torch.stack([p0, p1], dim=0).unsqueeze(1)
            weight = weight.repeat(model_channels, 1, 1)
            new_state_dict[new_key.replace('filter', 'conv.weight')] = weight
        elif 'downsample.lowpass.filter' in new_key:
            expanded = v.repeat(model_channels, 1, 1)
            new_state_dict[new_key.replace('filter', 'conv.weight')] = expanded
        else:
            new_state_dict[new_key] = v
    return new_state_dict

# ==============================================================================
# åŠ è½½ PyTorch æ¨¡å‹
# ==============================================================================

def load_pytorch_model(checkpoint_path):
    print("\nğŸ“¦ åŠ è½½åŸå§‹ PyTorch æ¨¡å‹...")
    
    # æ£€æµ‹é…ç½®
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('model', ckpt)
    
    model_channels = None
    for key in state_dict:
        if 'dec.conv_pre.weight' in key:
            model_channels = state_dict[key].shape[0]
            break
    
    if model_channels is None:
        raise ValueError("Cannot detect model channels")
    
    print(f"   æ£€æµ‹åˆ°é€šé“æ•°: {model_channels}")
    
    # åˆ›å»ºæ¨¡å‹
    model = SynthesizerTrn(
        spec_channels=128,
        segment_size=30,
        resblock="amp",
        resblock_kernel_sizes=[11],
        resblock_dilation_sizes=[[1, 3, 5]],
        upsample_initial_channel=model_channels
    )
    
    # åŠ è½½æƒé‡
    model.dec.remove_weight_norm()
    new_state_dict = convert_state_dict(state_dict, model_channels)
    model.load_state_dict(new_state_dict, strict=False)
    model.eval()
    
    print("   âœ… PyTorch æ¨¡å‹åŠ è½½æˆåŠŸ")
    return model

# ==============================================================================
# å¯¹æ¯”æµ‹è¯•
# ==============================================================================

def compare_models(checkpoint_path, onnx_path, test_length=16000, save_outputs=False):
    print("="*70)
    print("ğŸ” å¯¹æ¯” PyTorch æ¨¡å‹ vs ONNX æ¨¡å‹")
    print("="*70)
    
    # 1. åŠ è½½ PyTorch æ¨¡å‹
    try:
        pt_model = load_pytorch_model(checkpoint_path)
    except Exception as e:
        print(f"âŒ åŠ è½½ PyTorch æ¨¡å‹å¤±è´¥: {e}")
        return False
    
    # 2. åŠ è½½ ONNX æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½ ONNX æ¨¡å‹...")
    try:
        ort_session = ort.InferenceSession(onnx_path)
        input_name = ort_session.get_inputs()[0].name
        output_name = ort_session.get_outputs()[0].name
        print(f"   âœ… ONNX æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   è¾“å…¥: {input_name}")
        print(f"   è¾“å‡º: {output_name}")
    except Exception as e:
        print(f"âŒ åŠ è½½ ONNX æ¨¡å‹å¤±è´¥: {e}")
        return False
    
    # 3. åˆ›å»ºæµ‹è¯•æ•°æ®
    print(f"\nğŸ§ª åˆ›å»ºæµ‹è¯•æ•°æ® (é•¿åº¦={test_length})...")
    
    test_cases = []
    
    # æµ‹è¯•1: é™éŸ³
    silent_np = np.zeros((1, 1, test_length), dtype=np.float32)
    silent_torch = torch.from_numpy(silent_np)
    test_cases.append(("é™éŸ³", silent_torch, silent_np))
    
    # æµ‹è¯•2: æ­£å¼¦æ³¢ 440Hz
    t = np.linspace(0, test_length/16000, test_length)
    sine_wave = np.sin(2 * np.pi * 440 * t).astype(np.float32)
    sine_np = sine_wave.reshape(1, 1, -1)
    sine_torch = torch.from_numpy(sine_np)
    test_cases.append(("æ­£å¼¦æ³¢ (440 Hz)", sine_torch, sine_np))
    
    # æµ‹è¯•3: ç™½å™ªå£°
    noise_np = (np.random.randn(1, 1, test_length) * 0.1).astype(np.float32)
    noise_torch = torch.from_numpy(noise_np)
    test_cases.append(("ç™½å™ªå£°", noise_torch, noise_np))
    
    # æµ‹è¯•4: è„‰å†²
    impulse_np = np.zeros((1, 1, test_length), dtype=np.float32)
    impulse_np[0, 0, test_length//2] = 1.0
    impulse_torch = torch.from_numpy(impulse_np)
    test_cases.append(("è„‰å†²", impulse_torch, impulse_np))
    
    # 4. å¯¹æ¯”æ¨ç†ç»“æœ
    print("\nğŸ“Š å¯¹æ¯”æ¨ç†ç»“æœ...\n")
    print("-" * 70)
    
    all_passed = True
    max_differences = []
    
    for test_name, torch_input, np_input in test_cases:
        print(f"\næµ‹è¯•: {test_name}")
        print("-" * 70)
        
        try:
            # PyTorch æ¨ç†
            with torch.no_grad():
                pt_output = pt_model(torch_input).numpy()
            
            # ONNX æ¨ç†
            onnx_output = ort_session.run([output_name], {input_name: np_input})[0]
            
            # è®¡ç®—å·®å¼‚
            abs_diff = np.abs(pt_output - onnx_output)
            max_diff = abs_diff.max()
            mean_diff = abs_diff.mean()
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            pt_rms = np.sqrt(np.mean(pt_output**2))
            onnx_rms = np.sqrt(np.mean(onnx_output**2))
            pt_max = np.abs(pt_output).max()
            onnx_max = np.abs(onnx_output).max()
            
            # æ£€æŸ¥å¼‚å¸¸å€¼
            pt_has_nan = np.isnan(pt_output).any()
            onnx_has_nan = np.isnan(onnx_output).any()
            pt_has_inf = np.isinf(pt_output).any()
            onnx_has_inf = np.isinf(onnx_output).any()
            
            # åˆ¤æ–­æ˜¯å¦é€šè¿‡
            passed = (max_diff < 1e-3 and 
                     not pt_has_nan and not onnx_has_nan and 
                     not pt_has_inf and not onnx_has_inf)
            
            status = "âœ…" if passed else "âŒ"
            all_passed = all_passed and passed
            max_differences.append(max_diff)
            
            print(f"{status} å½¢çŠ¶å¯¹æ¯”:")
            print(f"   PyTorch:  {pt_output.shape}")
            print(f"   ONNX:     {onnx_output.shape}")
            
            print(f"\n{status} æ•°å€¼å¯¹æ¯”:")
            print(f"   æœ€å¤§å·®å¼‚:  {max_diff:.9f}")
            print(f"   å¹³å‡å·®å¼‚:  {mean_diff:.9f}")
            print(f"   ç›¸å¯¹è¯¯å·®:  {(max_diff / (pt_max + 1e-9)):.6f}")
            
            print(f"\n{status} ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   PyTorch  - RMS: {pt_rms:.6f}, Max: {pt_max:.6f}, NaN: {pt_has_nan}, Inf: {pt_has_inf}")
            print(f"   ONNX     - RMS: {onnx_rms:.6f}, Max: {onnx_max:.6f}, NaN: {onnx_has_nan}, Inf: {onnx_has_inf}")
            
            if not passed:
                print(f"\nâš ï¸  å·®å¼‚è¾ƒå¤§æˆ–æ£€æµ‹åˆ°å¼‚å¸¸å€¼ï¼")
                if max_diff >= 1e-3:
                    print(f"   - æœ€å¤§å·®å¼‚ {max_diff:.6f} è¶…è¿‡é˜ˆå€¼ 0.001")
                if pt_has_nan or onnx_has_nan:
                    print(f"   - æ£€æµ‹åˆ° NaN å€¼")
                if pt_has_inf or onnx_has_inf:
                    print(f"   - æ£€æµ‹åˆ° Inf å€¼")
            
            # ä¿å­˜è¾“å‡ºï¼ˆå¯é€‰ï¼‰
            if save_outputs:
                np.save(f'pt_output_{test_name.replace(" ", "_")}.npy', pt_output)
                np.save(f'onnx_output_{test_name.replace(" ", "_")}.npy', onnx_output)
                print(f"\nğŸ’¾ å·²ä¿å­˜è¾“å‡ºåˆ°æ–‡ä»¶")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            all_passed = False
    
    # 5. æœ€ç»ˆæ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š æ€»ç»“")
    print("="*70)
    
    if all_passed:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ONNX æ¨¡å‹ä¸ PyTorch æ¨¡å‹è¾“å‡ºä¸€è‡´")
        print(f"\nğŸ“ˆ æœ€å¤§å·®å¼‚èŒƒå›´: {min(max_differences):.9f} - {max(max_differences):.9f}")
        print("\nğŸ’¡ å¦‚æœ Unity ä¸­æ•ˆæœä»ç„¶ä¸å¥½ï¼Œå¯èƒ½çš„åŸå› :")
        print("   1. è¾“å…¥éŸ³é¢‘é‡‡æ ·ç‡ä¸æ˜¯ 16kHz")
        print("   2. ç«‹ä½“å£°åˆ°å•å£°é“è½¬æ¢æœ‰è¯¯")
        print("   3. è¾“å‡º AudioClip çš„é‡‡æ ·ç‡æœªè®¾ç½®ä¸º 48kHz")
        print("   4. éŸ³é¢‘æ•°æ®è¢«æ„å¤–ä¿®æ”¹ï¼ˆå¦‚è£å‰ªã€å½’ä¸€åŒ–ç­‰ï¼‰")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼ONNX æ¨¡å‹å¯èƒ½æœ‰é—®é¢˜")
        print(f"\nğŸ“ˆ æœ€å¤§å·®å¼‚: {max(max_differences):.9f}")
        print("\nğŸ”§ å»ºè®®:")
        print("   1. é‡æ–°å¯¼å‡ºæ¨¡å‹ï¼Œä½¿ç”¨æ›´ä½çš„ opset:")
        print("      python export_onnx.py --checkpoint pytorch_model_v2.bin --opset 13")
        print("   2. æ£€æŸ¥å¯¼å‡ºè„šæœ¬ä¸­çš„æ¨¡å‹å®šä¹‰æ˜¯å¦æ­£ç¡®")
        print("   3. å°è¯•ä½¿ç”¨ torch.onnx.export çš„ verbose=True æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
    
    print("="*70)
    
    return all_passed

# ==============================================================================
# ä¸»å‡½æ•°
# ==============================================================================

def main():
    parser = argparse.ArgumentParser(description='å¯¹æ¯”éªŒè¯ ONNX æ¨¡å‹ä¸ PyTorch æ¨¡å‹')
    parser.add_argument('--checkpoint', required=True, help='PyTorch checkpoint è·¯å¾„')
    parser.add_argument('--onnx', required=True, help='ONNX æ¨¡å‹è·¯å¾„')
    parser.add_argument('--test-length', type=int, default=16000, 
                       help='æµ‹è¯•éŸ³é¢‘é•¿åº¦ï¼ˆé»˜è®¤ 16000 = 1ç§’@16kHzï¼‰')
    parser.add_argument('--save-outputs', action='store_true',
                       help='ä¿å­˜è¾“å‡ºåˆ° .npy æ–‡ä»¶')
    args = parser.parse_args()
    
    if not Path(args.checkpoint).exists():
        print(f"âŒ Checkpoint ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    if not Path(args.onnx).exists():
        print(f"âŒ ONNX æ¨¡å‹ä¸å­˜åœ¨: {args.onnx}")
        return
    
    compare_models(args.checkpoint, args.onnx, args.test_length, args.save_outputs)

if __name__ == '__main__':
    main()